{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MosheWasserb/BestPracticeTextClassificationDistillation/blob/main/GPTtoTinyModels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How to use GPT to improve tiny models performance?**\n",
        "\n",
        "Data Augmentation based GPT (GPT-DA) was not shown to be a silver bullet to improve downstream tasks in low resource scenarios. The main reason is that GPT, although fine-tuned to specific training-data still generates new data that is too similar and is not able to expose the full distribution of the domain. On the other hand, less known is the fact that GPT generated data is very effective for training tiny models (few million parameters). In this notebook, I'll demonstrate how to deploy data-augmentation based GPT-Medium to improve tiny models accuracies.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8FfnUXvMl5lC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following table summarizes the results of fine-tuning several models on the SST2 dataset (part of the Glue benchamrk). All of the models were fine-tuned using the 66K samples available as SST2 training data in the GLUE benchmark, in addition, the tiny models (Fnet, Transformer, Bi-LSTM) were also trained using augmented data generated by GPT. As we can see the tiny models have shown significant accuracy boost while trained with GPT-DA. In this notebook I will explain how such results can be replicated.    "
      ],
      "metadata": {
        "id": "EKsqEUIzZIR9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Model**               | **#params[M]** | **Acc**  | **Training Data**          | \n",
        "|:-----------------------:|:-------:|:---------------:|:-----------------:|\n",
        "|  ***tiny-BERT kerasNLP**|    4    |   83.5          | 67K GLUE     \n",
        "|  ****tiny-BERT distill**|    4    |   83.4.         | 67K GLUE   \n",
        "|    **DistilBERT**       |    67   |   92.3.         | 67K GLUE     \n",
        "|    **Fnet**             |    0.74 |   81.5/88.7     | 67K/GPT-DA(800K) \n",
        "|    **Transformer**      |    0.79 |   81.2/87.5     | 67K/GPT-DA(800K)\n",
        "|       **Bi-LSTM**       |    0.66 |   82.9/**91.5** | 67K/GPT-DA(800K)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zaBCC4uSpFDx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*tiny-BERT kerasNLP - See https://keras.io/api/keras_nlp/models/ \n",
        "\n",
        "**tiny-BERT distill - See https://www.philschmid.de/knowledge-distillation-bert-transformers\n",
        "\n",
        "Fnet and Transformer network results are taken from https://keras.io/examples/nlp/fnet_classification_with_keras_nlp/"
      ],
      "metadata": {
        "id": "ulaw8ZE5ZO-w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recommended papers:\n",
        "\n",
        "1. \"A Few More Examples May Be Worth Billions of Parameters\" https://arxiv.org/pdf/2110.04374.pdf\n",
        "\n",
        "2. \"Data Augmentation using Pre-trained Transformer Models\" https://arxiv.org/abs/2003.02245\n",
        "\n",
        "3. \"GPT3Mix: Leveraging Large-scale Language Models for Text Augmentation\" https://arxiv.org/abs/2104.08826"
      ],
      "metadata": {
        "id": "8Mpt4dc8Pi5n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can download the Bi-LSTM model fine-tuned on GPT-DA from the Hugging Face repository (**https://huggingface.co/moshew/distilbilstm-finetuned-sst-2-english**)"
      ],
      "metadata": {
        "id": "jw6GAuxGTVj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install -q --upgrade keras-nlp tensorflow"
      ],
      "metadata": {
        "id": "89hjd4auWs5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Augmentation**"
      ],
      "metadata": {
        "id": "3NKhuawA3JE6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please note that steps 1-2 are only required if you dont have sufficient in-domain un-labeled data. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vFwf50qM0LPC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. For fine-tuning GPT model on the SST-2 dataset, Please follow J. Mamou's example here https://github.com/jmamou/data-augment\n"
      ],
      "metadata": {
        "id": "PLNS_6h3GsXQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A GPT-Medium model that was fine-tuned on SST-2 is avilable on the Hugging Face hub (**https://huggingface.co/jmamou/gpt2-medium-SST-2**)"
      ],
      "metadata": {
        "id": "qy_zreibHJSb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. In order to Generate samples, see J. Mamou's example here https://github.com/jmamou/data-augment. \n",
        "Please note it can take several hours even on strong machines since it is required to generate a large amount of data (~800K samples)."
      ],
      "metadata": {
        "id": "nnPSHj1mIJP-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. After we have generated a large set of un-labeled data, the next step is to label each data sample with the corresponding RoBERTa prediction.  "
      ],
      "metadata": {
        "id": "WnAOvUgWJL9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You can use the folllowing lines of code from S-BERT to generate the RoBERTa predictions \n",
        "#!pip install -U sentence-transformers\n",
        "#from sentence_transformers import CrossEncoder\n",
        "#model = CrossEncoder('philschmid/roberta-large-sst2', num_labels=2)\n",
        "#y_aug = model.predict(list(zip(x_aug)), batch_size=32)"
      ],
      "metadata": {
        "id": "aZyB0PNfpjP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pre-made GPT SST2 augmented data and corresponding prediction are available in Hugging Face's hub (**jmamou/augmented-glue-sst2**)"
      ],
      "metadata": {
        "id": "jODPsYk0nmEz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training and Evaluation**"
      ],
      "metadata": {
        "id": "SSJF6B9ZWQvO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's start and train our Tiny models"
      ],
      "metadata": {
        "id": "BVH2ISb6NJTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow import keras\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "keras.utils.set_random_seed(42)"
      ],
      "metadata": {
        "id": "gRZnUzhLTVSr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 512\n",
        "MAX_SEQUENCE_LENGTH = 64\n",
        "VOCAB_SIZE = 10000\n",
        "EMBED_DIM = 64\n",
        "INTERMEDIATE_DIM = 128\n",
        "\n",
        "NUM_CLASS = 2 \n",
        "\n",
        "value2hot = {\n",
        "  0: [1,0,],\n",
        "  1: [0,1]\n",
        "}"
      ],
      "metadata": {
        "id": "uBpNuEEGTV54"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load pre-made augmented data and RoBERTa's predictions \n",
        "from datasets import load_dataset\n",
        "\n",
        "# Training with GPT-2's augmented data\n",
        "sst2_aug = load_dataset(\"jmamou/augmented-glue-sst2\") \n",
        "y_train = np.array(sst2_aug['train']['prediction'])\n",
        "x_train=sst2_aug['train']['sentence']\n",
        "\n",
        "# If you want to train with GLUE dataset un-masked the following code lines\n",
        "#sst2_glue = load_dataset(\"glue\",\"sst2\") \n",
        "#y_train = np.array([value2hot[l] for l in sst2_glue['train']['label']])\n",
        "#x_train=sst2_glue['train']['sentence']"
      ],
      "metadata": {
        "id": "RU-2RENPAmi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load test/validation SST2 data\n",
        "sst2 = load_dataset(\"SetFit/sst2\")\n",
        "\n",
        "y_val = np.array([value2hot[l] for l in sst2['validation']['label']])\n",
        "y_test = np.array([value2hot[l] for l in sst2['test']['label']])\n",
        "\n",
        "x_val=sst2['validation']['text']\n",
        "x_test=sst2['test']['text']"
      ],
      "metadata": {
        "id": "LXfW4meVTYnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Bert-tiny fine-tuned on SST2"
      ],
      "metadata": {
        "id": "ANCfO8AuToRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import keras_nlp\n",
        "classifier = keras_nlp.models.BertClassifier.from_preset(\"bert_tiny_en_uncased_sst2\")\n",
        "y_pred = np.argmax(classifier.predict(x_test), axis=1)\n",
        "accuracy_score(y_pred, sst2['test']['label'])"
      ],
      "metadata": {
        "id": "hlHSyk0eTo-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple word tokenizer"
      ],
      "metadata": {
        "id": "Appb9R0OUBO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize our training data\n",
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "word_index = tokenizer.word_index\n",
        "num_words = min(VOCAB_SIZE, len(word_index) + 1)"
      ],
      "metadata": {
        "id": "fZUPgzOlUCy4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizer_padding(sentence):\n",
        "    sentence_sequences = tokenizer.texts_to_sequences(sentence)\n",
        "    sentence_padded = pad_sequences(sentence_sequences, padding='post', truncating='post', maxlen=MAX_SEQUENCE_LENGTH)\n",
        "    return (sentence_padded)\n",
        "\n",
        "X_train = tokenizer_padding(x_train)\n",
        "X_val = tokenizer_padding(x_val)\n",
        "X_test = tokenizer_padding(x_test)"
      ],
      "metadata": {
        "id": "7wLGPkxyUFH0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test FNet"
      ],
      "metadata": {
        "id": "dtgvSUx5UNnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = keras.Input(shape=(None,), dtype=\"int64\", name=\"input_ids\")\n",
        "\n",
        "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "    vocabulary_size=VOCAB_SIZE,\n",
        "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "    embedding_dim=EMBED_DIM,\n",
        "    mask_zero=True,\n",
        ")(input_ids)\n",
        "\n",
        "x = keras_nlp.layers.FNetEncoder(intermediate_dim=INTERMEDIATE_DIM)(inputs=x)\n",
        "x = keras_nlp.layers.FNetEncoder(intermediate_dim=INTERMEDIATE_DIM)(inputs=x)\n",
        "x = keras_nlp.layers.FNetEncoder(intermediate_dim=INTERMEDIATE_DIM)(inputs=x)\n",
        "\n",
        "\n",
        "x = keras.layers.GlobalAveragePooling1D()(x)\n",
        "x = keras.layers.Dropout(0.1)(x)\n",
        "outputs = keras.layers.Dense(NUM_CLASS, activation=\"softmax\")(x)\n",
        "\n",
        "fnet_classifier = keras.Model(input_ids, outputs, name=\"fnet_classifier\")\n",
        "\n",
        "fnet_classifier.summary()\n",
        "fnet_classifier.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss=\"KLD\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "fnet_classifier.fit(X_train, y_train, epochs=3, validation_data=[X_test, y_test], batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvpo1ZpWUO6A",
        "outputId": "ed8ef692-f68e-4838-bd83-2b7b6ae8507c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"fnet_classifier\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_ids (InputLayer)      [(None, None)]            0         \n",
            "                                                                 \n",
            " token_and_position_embeddin  (None, None, 64)         644096    \n",
            " g (TokenAndPositionEmbeddin                                     \n",
            " g)                                                              \n",
            "                                                                 \n",
            " f_net_encoder (FNetEncoder)  (None, None, 64)         16832     \n",
            "                                                                 \n",
            " f_net_encoder_1 (FNetEncode  (None, None, 64)         16832     \n",
            " r)                                                              \n",
            "                                                                 \n",
            " f_net_encoder_2 (FNetEncode  (None, None, 64)         16832     \n",
            " r)                                                              \n",
            "                                                                 \n",
            " global_average_pooling1d (G  (None, 64)               0         \n",
            " lobalAveragePooling1D)                                          \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2)                 130       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 694,722\n",
            "Trainable params: 694,722\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/3\n",
            "1557/1557 [==============================] - 101s 61ms/step - loss: 0.3151 - accuracy: 0.7685 - val_loss: 0.3087 - val_accuracy: 0.8622\n",
            "Epoch 2/3\n",
            "1557/1557 [==============================] - 53s 34ms/step - loss: 0.2376 - accuracy: 0.8264 - val_loss: 0.2760 - val_accuracy: 0.8737\n",
            "Epoch 3/3\n",
            "1557/1557 [==============================] - 49s 32ms/step - loss: 0.2129 - accuracy: 0.8439 - val_loss: 0.2657 - val_accuracy: 0.8841\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe7c640af10>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer"
      ],
      "metadata": {
        "id": "3QpGG8rXUSkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_HEADS = 2\n",
        "input_ids = keras.Input(shape=(None,), dtype=\"int64\", name=\"input_ids\")\n",
        "\n",
        "\n",
        "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "    vocabulary_size=VOCAB_SIZE,\n",
        "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "    embedding_dim=EMBED_DIM,\n",
        "    mask_zero=False,\n",
        ")(input_ids)\n",
        "\n",
        "x = keras_nlp.layers.TransformerEncoder(\n",
        "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
        ")(inputs=x)\n",
        "x = keras_nlp.layers.TransformerEncoder(\n",
        "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
        ")(inputs=x)\n",
        "x = keras_nlp.layers.TransformerEncoder(\n",
        "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
        ")(inputs=x)\n",
        "\n",
        "\n",
        "x = keras.layers.GlobalAveragePooling1D()(x)\n",
        "x = keras.layers.Dropout(0.1)(x)\n",
        "outputs = keras.layers.Dense(NUM_CLASS, activation=\"softmax\")(x)\n",
        "\n",
        "transformer_classifier = keras.Model(input_ids, outputs,name=\"transformer_classifier\")\n",
        "\n",
        "\n",
        "transformer_classifier.summary()\n",
        "transformer_classifier.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss=\"KLD\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "transformer_classifier.fit(X_train, y_train, epochs=3, validation_data=[X_test, y_test], batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyPCaQwQUVJb",
        "outputId": "fe7df122-0942-43d5-f0f4-4fd96ac383b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer_classifier\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_ids (InputLayer)      [(None, None)]            0         \n",
            "                                                                 \n",
            " token_and_position_embeddin  (None, None, 64)         644096    \n",
            " g_1 (TokenAndPositionEmbedd                                     \n",
            " ing)                                                            \n",
            "                                                                 \n",
            " transformer_encoder (Transf  (None, None, 64)         33472     \n",
            " ormerEncoder)                                                   \n",
            "                                                                 \n",
            " transformer_encoder_1 (Tran  (None, None, 64)         33472     \n",
            " sformerEncoder)                                                 \n",
            "                                                                 \n",
            " transformer_encoder_2 (Tran  (None, None, 64)         33472     \n",
            " sformerEncoder)                                                 \n",
            "                                                                 \n",
            " global_average_pooling1d_1   (None, 64)               0         \n",
            " (GlobalAveragePooling1D)                                        \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 130       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 744,642\n",
            "Trainable params: 744,642\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/3\n",
            "1557/1557 [==============================] - 97s 57ms/step - loss: 0.3251 - accuracy: 0.7633 - val_loss: 0.3438 - val_accuracy: 0.8523\n",
            "Epoch 2/3\n",
            "1557/1557 [==============================] - 68s 44ms/step - loss: 0.2708 - accuracy: 0.8040 - val_loss: 0.3076 - val_accuracy: 0.8720\n",
            "Epoch 3/3\n",
            "1557/1557 [==============================] - 67s 43ms/step - loss: 0.2481 - accuracy: 0.8202 - val_loss: 0.2906 - val_accuracy: 0.8699\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe7219ff220>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bi-LSTM + Glove"
      ],
      "metadata": {
        "id": "kT4Z35MxUaNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load Glove's files\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip glove.6B.50d.txt"
      ],
      "metadata": {
        "id": "1ePQckzcUbaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load Glove \n",
        "# define dict to hold a word and its vector\n",
        "glove = {}\n",
        "# read the word embeddings file ~820MB\n",
        "f = open('glove.6B.50d.txt', encoding='utf-8')\n",
        "\n",
        "for line in f:\n",
        "  values = line.split()\n",
        "  word = values[0]\n",
        "  coefs = np.asarray(values[1:], dtype='float32')\n",
        "  glove[word] = coefs\n",
        "f.close()\n",
        "# check the length\n",
        "len(glove) # 400000\n",
        "\n",
        "j=0\n",
        "embedding_matrix = np.zeros((num_words, 50))\n",
        "for word, i in word_index.items():\n",
        "  if i >= VOCAB_SIZE:\n",
        "      continue\n",
        "  embedding_vector = glove.get(word)\n",
        "  if embedding_vector is not None:\n",
        "# words not found in embedding index will be all-zeros.\n",
        "      embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "G2x1DrQ7Ud1V"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = keras.layers.Embedding(input_dim=VOCAB_SIZE,\n",
        "                            output_dim=50,\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            embeddings_initializer=keras.initializers.Constant(embedding_matrix), #Glove\n",
        "                            trainable=True)\n",
        "\n",
        "input_ids = keras.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int64\")\n",
        "embedded_sequences = embedding_layer(input_ids)\n",
        "x = keras.layers.Bidirectional(keras.layers.LSTM(64, dropout=0.2, return_sequences=True))(embedded_sequences)\n",
        "x = keras.layers.Bidirectional(keras.layers.LSTM(64))(x)\n",
        "x = keras.layers.Dropout(0.2)(x)\n",
        "outputs = keras.layers.Dense(NUM_CLASS, activation='softmax')(x)\n",
        "\n",
        "bilstm_classifier = keras.Model(input_ids, outputs)\n",
        "\n",
        "bilstm_classifier.summary()\n",
        "bilstm_classifier.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss=\"KLD\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "bilstm_classifier.fit(X_train, y_train, epochs=10, validation_data=[X_test, y_test], batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "pTP5jLbtUgZ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee6af373-6e77-4c1c-bbfc-1fba19c037ec"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None, 64)]              0         \n",
            "                                                                 \n",
            " embedding_4 (Embedding)     (None, 64, 50)            500000    \n",
            "                                                                 \n",
            " bidirectional_8 (Bidirectio  (None, 64, 128)          58880     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " bidirectional_9 (Bidirectio  (None, 128)              98816     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 657,954\n",
            "Trainable params: 657,954\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "1557/1557 [==============================] - 73s 43ms/step - loss: 0.3089 - accuracy: 0.7773 - val_loss: 0.2953 - val_accuracy: 0.8704\n",
            "Epoch 2/10\n",
            "1557/1557 [==============================] - 43s 28ms/step - loss: 0.2423 - accuracy: 0.8225 - val_loss: 0.2544 - val_accuracy: 0.8946\n",
            "Epoch 3/10\n",
            "1557/1557 [==============================] - 42s 27ms/step - loss: 0.2249 - accuracy: 0.8339 - val_loss: 0.2580 - val_accuracy: 0.8885\n",
            "Epoch 4/10\n",
            "1557/1557 [==============================] - 40s 26ms/step - loss: 0.2142 - accuracy: 0.8407 - val_loss: 0.2447 - val_accuracy: 0.9017\n",
            "Epoch 5/10\n",
            "1557/1557 [==============================] - 41s 26ms/step - loss: 0.2060 - accuracy: 0.8460 - val_loss: 0.2433 - val_accuracy: 0.8979\n",
            "Epoch 6/10\n",
            "1557/1557 [==============================] - 40s 26ms/step - loss: 0.1992 - accuracy: 0.8502 - val_loss: 0.2402 - val_accuracy: 0.9050\n",
            "Epoch 7/10\n",
            "1557/1557 [==============================] - 41s 26ms/step - loss: 0.1935 - accuracy: 0.8539 - val_loss: 0.2298 - val_accuracy: 0.9066\n",
            "Epoch 8/10\n",
            "1557/1557 [==============================] - 40s 26ms/step - loss: 0.1883 - accuracy: 0.8575 - val_loss: 0.2239 - val_accuracy: 0.9121\n",
            "Epoch 9/10\n",
            "1557/1557 [==============================] - 40s 26ms/step - loss: 0.1837 - accuracy: 0.8603 - val_loss: 0.2237 - val_accuracy: 0.9110\n",
            "Epoch 10/10\n",
            "1557/1557 [==============================] - 40s 26ms/step - loss: 0.1794 - accuracy: 0.8637 - val_loss: 0.2204 - val_accuracy: 0.9154\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd8ef1a1e50>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ]
}