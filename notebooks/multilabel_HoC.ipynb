{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IYf85KnrWoH"
      },
      "source": [
        "# SetFit SOTA for Bio Text Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DM_RojZL3XDF"
      },
      "source": [
        "SetFit is a great practical tool for few shot text classification, but did you know that you can fine-tune a vanilla SetFit for full-shot text classification and outperform models that were pre-trained from scratch using domain data?\n",
        "Here we show such example in the Biological domain, where SetFit outperforms most of the models that were trained from scratch on Biological data, while being more efficient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS_qaldP3Ths"
      },
      "source": [
        "The following table summarizes the results of different models on the HoC* dataset. All of the biological models were first pre-trained using in-domain biological data and in addition were fine-tuned given the HoC training data in the BLUE benchmark. SetFit was not pre-trained using biological data, it is based on a general pre-trained sentence transformer model (MSFT's mpnet) and was solely fine-tuned on the HoC training data. As shown in the table, SetFit surpasses the Bio models and achieves comparable performance to the 347M BioGPT, which is the SOTA model for the Bio domain, while being 3x smaller: https://analyticsindiamag.com/microsoft-launches-biogpt-the-chatgpt-of-lifescience/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLPGtW9B3I2J"
      },
      "source": [
        "| **Model**               | **#params[M]** | **F1**  | **Pre-train Data**          |\n",
        "|:-----------------------:|:-------:|:---------------:|:-----------------:|\n",
        "|  **BioBERT[1]**|    110    |   81.5          | Bio     \n",
        "|  **PubMedBERT[2]**|    340    |   82.7          | Bio   \n",
        "|    **BioLinkBERT[3]**       |    340   |   84.9          | Bio     \n",
        "|    **GPT-2**             |    355 |   81.8     | General\n",
        "|    **BioGPT[4]**      |    347 |   85.1     | Bio\n",
        "|       **SetFit**       |    105 |   **85.1** | General\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arGWgoRK3BlE"
      },
      "source": [
        "Refrences:\n",
        "\n",
        "[1] Domain-specific\n",
        "language model pretraining for biomedical natural language\n",
        "processing\" https://arxiv.org/abs/2007.15779\n",
        "\n",
        "[2] BioBERT: a pre-trained biomedical language representation\n",
        "model for biomedical text mining\" https://arxiv.org/abs/1901.08746\n",
        "\n",
        "[3] LinkBERT: Pretraining Language Models with Document Links https://arxiv.org/abs/2203.15827\n",
        "\n",
        "[4] BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining\" https://arxiv.org/abs/2210.10341\n",
        "\n",
        "[5] Automatic semantic classification of scientific literature according to\n",
        "the hallmarks of cancer. https://academic.oup.com/bioinformatics/article/32/3/432/1743783\n",
        "\n",
        "[6]  An\n",
        "evaluation of BERT and ELMo on ten benchmarking\n",
        "datasets https://arxiv.org/abs/1906.05474\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_lI2PxcPOAJ"
      },
      "source": [
        "*HoC (the Hallmarks of Cancers corpus) consists of 1580\n",
        "PubMed abstracts manually annotated at sentence level by\n",
        "experts with ten currently known hallmarks of cancer [5]. We follow the same training/test split as in [6]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zelvDdXVvV4a"
      },
      "source": [
        "### SetFit Multilabel HoC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DaL_TsPWrcI3"
      },
      "outputs": [],
      "source": [
        "!pip install setfit==0.7.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etBVpBqJrpd-"
      },
      "source": [
        "Load the HoC dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "wTZgoQPPrn7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8abe971-566d-4ce9-a5eb-b92d67c2f0e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-19 17:16:51--  https://github.com/ncbi-nlp/BLUE_Benchmark/releases/download/0.1/data_v0.1.zip\n",
            "Resolving github.com (github.com)... 20.27.177.113\n",
            "Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/190591943/a5acee80-7695-11ea-99ea-8aeab034b689?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-07-19T18%3A03%3A01Z&rscd=attachment%3B+filename%3Ddata_v0.1.zip&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-07-19T17%3A02%3A13Z&ske=2025-07-19T18%3A03%3A01Z&sks=b&skv=2018-11-09&sig=Tf5FqMSwAOKJhSak3BV1iB3Y%2F4uB%2BZgyjGpcWGVSL8Q%3D&jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1Mjk0NTcxMSwibmJmIjoxNzUyOTQ1NDExLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.t4oI6rZIF-66pBBgIxsigABVMu1OBVVH3umZ5IAU8v0&response-content-disposition=attachment%3B%20filename%3Ddata_v0.1.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-07-19 17:16:52--  https://release-assets.githubusercontent.com/github-production-release-asset/190591943/a5acee80-7695-11ea-99ea-8aeab034b689?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-07-19T18%3A03%3A01Z&rscd=attachment%3B+filename%3Ddata_v0.1.zip&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-07-19T17%3A02%3A13Z&ske=2025-07-19T18%3A03%3A01Z&sks=b&skv=2018-11-09&sig=Tf5FqMSwAOKJhSak3BV1iB3Y%2F4uB%2BZgyjGpcWGVSL8Q%3D&jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1Mjk0NTcxMSwibmJmIjoxNzUyOTQ1NDExLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.t4oI6rZIF-66pBBgIxsigABVMu1OBVVH3umZ5IAU8v0&response-content-disposition=attachment%3B%20filename%3Ddata_v0.1.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10653585 (10M) [application/octet-stream]\n",
            "Saving to: ‘data_v0.1.zip.1’\n",
            "\n",
            "data_v0.1.zip.1     100%[===================>]  10.16M  15.9MB/s    in 0.6s    \n",
            "\n",
            "2025-07-19 17:16:53 (15.9 MB/s) - ‘data_v0.1.zip.1’ saved [10653585/10653585]\n",
            "\n",
            "Archive:  data_v0.1.zip\n",
            "replace data/BC5CDR/bc5cdr-chem-test.tsv? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/ncbi-nlp/BLUE_Benchmark/releases/download/0.1/data_v0.1.zip\n",
        "!unzip data_v0.1.zip\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Read train/test files\n",
        "test_df = pd.read_csv('/content/data/hoc/test.tsv', sep='\\t')\n",
        "train_df = pd.read_csv('/content/data/hoc/train.tsv', sep='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "WbcTmX31sqvY"
      },
      "outputs": [],
      "source": [
        "LABELS = ['activating invasion and metastasis', 'avoiding immune destruction',\n",
        "          'cellular energetics', 'enabling replicative immortality', 'evading growth suppressors',\n",
        "          'genomic instability and mutation', 'inducing angiogenesis', 'resisting cell death',\n",
        "          'sustaining proliferative signaling', 'tumor promoting inflammation']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "kvlT2j6zwfGM"
      },
      "outputs": [],
      "source": [
        "# Convert labels to hotvec multilabel format (similar to scikit-learn)\n",
        "def hotvec_multilabel(true_df):\n",
        "    data = {}\n",
        "\n",
        "    for i in range(len(true_df)):\n",
        "        true_row = true_df.iloc[i]\n",
        "\n",
        "        key = true_row['index']\n",
        "\n",
        "        data[key] = set()\n",
        "\n",
        "        if not pd.isna(true_row['labels']):\n",
        "            for l in true_row['labels'].split(','):\n",
        "                data[key].add(LABELS.index(l))\n",
        "\n",
        "    y_hotvec = []\n",
        "    for k, (true) in data.items():\n",
        "        t = [0] * len(LABELS)\n",
        "        for i in true:\n",
        "            t[i] = 1\n",
        "\n",
        "        y_hotvec.append(t)\n",
        "\n",
        "    y_hotvec = np.array(y_hotvec)\n",
        "\n",
        "    return y_hotvec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzN7ATsqsuFn"
      },
      "source": [
        "### SetFit Multilabel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pbu3EwqNsx4W",
        "outputId": "9086c449-54ce-4186-df0a-c60235b1f9e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:`SentenceTransformer._target_device` has been deprecated, please use `SentenceTransformer.device` instead.\n",
            "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "import evaluate\n",
        "from setfit import SetFitModel, SetFitTrainer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "#model = SentenceTransformer(st_model, config_kwargs={\"use_memory_efficient_attention\": False, \"unpad_inputs\": False}, trust_remote_code=True)\n",
        "\n",
        "model = SetFitModel.from_pretrained(\n",
        "    \"BAAI/bge-small-en\",\n",
        "    # \"all-MiniLM-L6-v2\", \"paraphrase-mpnet-base-v2 f1 36.3/84\", \"BAAI/bge-small-en 40.2/\", \"NeuML/bioclinical-modernbert-base-embeddings 74.7 81\"\n",
        "\n",
        "    multi_target_strategy=\"multi-output\",     # one-vs-rest; multi-output; classifier-chain\n",
        "    config_kwargs={\"use_memory_efficient_attention\": False, \"unpad_inputs\": False}\n",
        ")\n",
        "\n",
        "#\"NeuML/pubmedbert-base-embeddings\",\n",
        "#\"BAAI/bge-base-en\n",
        "#\"NeuML/bioclinical-modernbert-base-embeddings\n",
        "#nomic-ai/modernbert-embed-base\n",
        "#Qwen/Qwen3-Embedding-0.6B\n",
        "#NeuML/pubmedbert-base-splade # model = SparseEncoder(\"neuml/pubmedbert-base-splade\")\n",
        "\n",
        "#model = SetFitModel.from_pretrained(model_id, multi_target_strategy=\"one-vs-rest\")\n",
        "\n",
        "multilabel_f1_metric = evaluate.load(\"f1\", \"multilabel\")\n",
        "multilabel_accuracy_metric = evaluate.load(\"accuracy\", \"multilabel\")\n",
        "\n",
        "# f1/accuracy sentence level\n",
        "def compute_metrics(y_pred, y_test):\n",
        "    return {\n",
        "        \"f1\": multilabel_f1_metric.compute(predictions=y_pred, references=y_test, average=\"micro\")[\"f1\"],\n",
        "        \"accuracy\": multilabel_accuracy_metric.compute(predictions=y_pred, references=y_test)[\"accuracy\"],\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "Atx4T90ts4L5"
      },
      "outputs": [],
      "source": [
        "eval_dataset = Dataset.from_dict({\"text\": test_df['sentence'], \"label\": hotvec_multilabel(test_df)})\n",
        "train_dataset = Dataset.from_dict({\"text\": train_df['sentence'], \"label\": hotvec_multilabel(train_df)})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_sampled_random = train_dataset.shuffle(seed=1).select(range(1000))"
      ],
      "metadata": {
        "id": "XSo176gPJsrl"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SetFitTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    metric=compute_metrics,\n",
        "    num_iterations=5\n",
        ")"
      ],
      "metadata": {
        "id": "5TEq_MreygJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJu5u-vltPMz"
      },
      "outputs": [],
      "source": [
        "trainer.train()\n",
        "metrics = trainer.evaluate()\n",
        "print(metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sci kit classifier"
      ],
      "metadata": {
        "id": "dTP235EhLvVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer(\"Qwen/Qwen3-Embedding-0.6B\")"
      ],
      "metadata": {
        "id": "H9uOoMeQQz3D"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = train_dataset_sampled_random['text']\n",
        "y_train = train_dataset_sampled_random['label']\n",
        "\n",
        "x_test = eval_dataset['text']\n",
        "y_test = eval_dataset['label']\n",
        "\n",
        "X_train = model.encode(x_train)\n",
        "X_test = model.encode(x_test)"
      ],
      "metadata": {
        "id": "65tgW6GxLyoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "OneVsRest_classifier = OneVsRestClassifier(LogisticRegression(solver='liblinear')) # Wrap LogisticRegression in OneVsRestClassifier\n",
        "\n",
        "# Train the classifier\n",
        "# X_train should be your features (e.g., sentence embeddings)\n",
        "# y_train should be your multi-hot encoded labels\n",
        "OneVsRest_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = OneVsRest_classifier.predict(X_test)\n",
        "\n",
        "micro_f1 = multilabel_f1_metric.compute(predictions=y_pred, references=y_test, average=\"micro\")[\"f1\"]\n",
        "# weighted\", \"macro\"\n",
        "print(f\"Micro F1 Score: {micro_f1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swH9SlziWcor",
        "outputId": "7df10768-f16f-4286-a52c-f2c2a530d43d"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Micro F1 Score: 0.001962708537782139\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.multioutput import ClassifierChain\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "\n",
        "# Initialize ClassifierChain with a base classifier\n",
        "# You can use LogisticRegression, SGDClassifier, or other binary classifiers\n",
        "# The order of the chain can be randomized or fixed\n",
        "base_classifier = LogisticRegression(solver='liblinear')\n",
        "chain_classifier = ClassifierChain(base_classifier, random_state=42) # random_state for reproducibility of chain order\n",
        "\n",
        "# Train the ClassifierChain model\n",
        "chain_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = chain_classifier.predict(X_test)\n",
        "\n",
        "micro_f1 = multilabel_f1_metric.compute(predictions=y_pred, references=y_test, average=\"micro\")[\"f1\"]\n",
        "# weighted\", \"macro\"\n",
        "print(f\"Micro F1 Score: {micro_f1}\")"
      ],
      "metadata": {
        "id": "_rGGlarYb7u7",
        "outputId": "6d3cb61b-afa3-49a0-8955-1252ad868ca4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Micro F1 Score: 0.001962708537782139\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jwAW-xagMkMr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "409BjocBtkkf"
      },
      "source": [
        "### Evaluation of BLUE's HoC F1 (abstract level)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifJTpGsItsXJ"
      },
      "source": [
        "Support functions refactored from https://github.com/ncbi-nlp/BLUE_Benchmark\n",
        "can be downloaded at https://github.com/ncbi-nlp/BLUE_Benchmark/releases/tag/0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "hbNss08Wtkkh"
      },
      "outputs": [],
      "source": [
        "def divide(x, y):\n",
        "    return np.true_divide(x, y, out=np.zeros_like(x, dtype=np.float64), where=y != 0)\n",
        "\n",
        "def get_p_r_f_arrary(test_predict_label, test_true_label):\n",
        "    num, cat = test_predict_label.shape\n",
        "    acc_list = []\n",
        "    prc_list = []\n",
        "    rec_list = []\n",
        "    f_score_list = []\n",
        "    for i in range(num):\n",
        "        label_pred_set = set()\n",
        "        label_gold_set = set()\n",
        "\n",
        "        for j in range(cat):\n",
        "            if test_predict_label[i, j] == 1:\n",
        "                label_pred_set.add(j)\n",
        "            if test_true_label[i, j] == 1:\n",
        "                label_gold_set.add(j)\n",
        "\n",
        "        uni_set = label_gold_set.union(label_pred_set)\n",
        "        intersec_set = label_gold_set.intersection(label_pred_set)\n",
        "\n",
        "        tt = len(intersec_set)\n",
        "        if len(label_pred_set) == 0:\n",
        "            prc = 0\n",
        "        else:\n",
        "            prc = tt / len(label_pred_set)\n",
        "\n",
        "        acc = tt / len(uni_set)\n",
        "\n",
        "        rec = tt / len(label_gold_set)\n",
        "\n",
        "        if prc == 0 and rec == 0:\n",
        "            f_score = 0\n",
        "        else:\n",
        "            f_score = 2 * prc * rec / (prc + rec)\n",
        "\n",
        "        acc_list.append(acc)\n",
        "        prc_list.append(prc)\n",
        "        rec_list.append(rec)\n",
        "        f_score_list.append(f_score)\n",
        "\n",
        "    mean_prc = np.mean(prc_list)\n",
        "    mean_rec = np.mean(rec_list)\n",
        "    f_score = divide(2 * mean_prc * mean_rec, (mean_prc + mean_rec))\n",
        "    return mean_prc, mean_rec, f_score\n",
        "\n",
        "def eval_hoc(true_df, pred_df):\n",
        "    data = {}\n",
        "\n",
        "    assert len(true_df) == len(pred_df), \\\n",
        "        f'Gold line no {len(true_df)} vs Prediction line no {len(pred_df)}'\n",
        "\n",
        "    for i in range(len(true_df)):\n",
        "        true_row = true_df.iloc[i]\n",
        "        pred_row = pred_df.iloc[i]\n",
        "        assert true_row['index'] == pred_row['index'], \\\n",
        "            'Index does not match @{}: {} vs {}'.format(i, true_row['index'], pred_row['index'])\n",
        "\n",
        "        key = true_row['index'][:true_row['index'].find('_')]\n",
        "        if key not in data:\n",
        "            data[key] = (set(), set())\n",
        "\n",
        "        if not pd.isna(true_row['labels']):\n",
        "            for l in true_row['labels'].split(','):\n",
        "                data[key][0].add(LABELS.index(l))\n",
        "\n",
        "        if not pd.isna(pred_row['labels']):\n",
        "            for l in pred_row['labels'].split(','):\n",
        "                data[key][1].add(LABELS.index(l))\n",
        "\n",
        "    assert len(data) == 315, 'There are 315 documents in the test set: %d' % len(data)\n",
        "\n",
        "    y_test = []\n",
        "    y_pred = []\n",
        "    for k, (true, pred) in data.items():\n",
        "        t = [0] * len(LABELS)\n",
        "        for i in true:\n",
        "            t[i] = 1\n",
        "\n",
        "        p = [0] * len(LABELS)\n",
        "        for i in pred:\n",
        "            p[i] = 1\n",
        "\n",
        "        y_test.append(t)\n",
        "        y_pred.append(p)\n",
        "\n",
        "    y_test = np.array(y_test)\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    r, p, f1 = get_p_r_f_arrary(y_pred, y_test)\n",
        "    print('Precision: {:.1f}'.format(p*100))\n",
        "    print('Recall   : {:.1f}'.format(r*100))\n",
        "    print('F1       : {:.1f}'.format(f1*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PE_A3qdbtxns"
      },
      "source": [
        "#### Evaluate on test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "0A6rzsLKt5wC"
      },
      "outputs": [],
      "source": [
        "#test_predict_label = trainer.model.predict(test_df['sentence'])\n",
        "x_TEST = test_df['sentence']\n",
        "X_TEST = model.encode(x_TEST)\n",
        "test_predict_label = chain_classifier.predict(X_TEST)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "ya4Ps34ut8Dm"
      },
      "outputs": [],
      "source": [
        "# Convert hotvec multilabel to actual labels\n",
        "num, cat = test_predict_label.shape\n",
        "sentence_list = []\n",
        "for i in range(num):\n",
        "    sentence_set = set()\n",
        "    for j in range(cat):\n",
        "        if test_predict_label[i, j] == 1:\n",
        "            sentence_set.add(LABELS[j])\n",
        "    sentence_list.append(','.join(sentence_set))\n",
        "\n",
        "# Reformat for HoC evaluation\n",
        "pred_df = test_df\n",
        "pred_df = pred_df.assign(labels = sentence_list)\n",
        "pred_df['labels'] = pred_df['labels'].replace({'':np.nan})\n",
        "test_df['labels'] = test_df['labels'].replace({'':np.nan})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_5Typrvut_z"
      },
      "source": [
        "#### Evaluate F1 (abstract level)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#bge_base\n",
        "eval_hoc(test_df, pred_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiI5vHqzz4z8",
        "outputId": "29222938-7d91-43b7-8726-784a2f099e22"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 41.2\n",
            "Recall   : 50.7\n",
            "F1       : 45.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZdQNVDPuyDJ",
        "outputId": "a29fd333-f0f1-4e3d-b691-c38ec46e1f1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 72.5\n",
            "Recall   : 73.0\n",
            "F1       : 72.8\n"
          ]
        }
      ],
      "source": [
        "# \"NeuML/bioclinical-modernbert-base-embeddings\n",
        "eval_hoc(test_df, pred_df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#NeuML/pubmedbert-base-embeddings\n",
        "eval_hoc(test_df, pred_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABqv8ZY1-YaC",
        "outputId": "6c4e2e82-c36f-4299-8d85-424c38b986be"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 73.3\n",
            "Recall   : 76.4\n",
            "F1       : 74.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#omic-ai/modernbert-embed-base\n",
        "eval_hoc(test_df, pred_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DYdSxm4_v8R",
        "outputId": "96f267f1-bf61-4012-fc00-5fdd5aa08bca"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 44.0\n",
            "Recall   : 52.7\n",
            "F1       : 48.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#omic-ai/modernbert-embed-base\n",
        "eval_hoc(test_df, pred_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wM6qMeKhC7x_",
        "outputId": "e26e20c2-fdc3-4711-af95-756e8215bd2d"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 87.2\n",
            "Recall   : 84.7\n",
            "F1       : 86.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#NeuML/pubmedbert-base-embeddings.  defualt num_itr only 1000 samples\n",
        "eval_hoc(test_df, pred_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2vDi4kZF1Le",
        "outputId": "e708237e-6a74-4357-b69f-c71b528ccac0"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.3\n",
            "Recall   : 0.3\n",
            "F1       : 0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bIE0BAt2TYEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " [seed-42, itr=defualt, 5], seed = 42, seed=0, seed=1\n",
        "\n",
        " \"NeuML/pubmedbert-base-embeddings\",74.7 79.1 81.7 77.3 = 79.4 (85.2) 54.5 sck\n",
        "\n",
        "\"BAAI/bge-base-en 45.4 66 63.8 65.5 = 65.1 (82.5) 0\n",
        "\n",
        "\"NeuML/bioclinical-modernbert-base-embeddings 73.2 78.7 75.9 73.9 = 76.2 (81) 55 sck\n",
        "\n",
        "nomic-ai/modernbert-embed-base  48 58.7 55.7 57.9 = 57.4 (82.5) 0.3\n",
        "\n",
        "Qwen/Qwen3-Embedding-0.6B 69.9 64.5 65.4 71.7 = 67.2 (83.3)\n",
        "\n",
        "\n",
        "emilyalsentzer/Bio_ClinicalBERT  63.1 x x"
      ],
      "metadata": {
        "id": "aVzv-2UkTgYr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "980717ba"
      },
      "source": [
        "from sklearn.multioutput import ClassifierChain\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "\n",
        "# Initialize ClassifierChain with a base classifier\n",
        "# You can use LogisticRegression, SGDClassifier, or other binary classifiers\n",
        "# The order of the chain can be randomized or fixed\n",
        "base_classifier = LogisticRegression(solver='liblinear')\n",
        "chain_classifier = ClassifierChain(base_classifier, random_state=42) # random_state for reproducibility of chain order\n",
        "\n",
        "# Train the ClassifierChain model\n",
        "chain_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "# The predict method returns multi-hot encoded predictions\n",
        "y_pred_chain = chain_classifier.predict(X_test)\n",
        "\n",
        "# Display the shape and some examples of the predictions\n",
        "print(\"Shape of multi-hot encoded predictions from ClassifierChain:\", y_pred_chain.shape)\n",
        "print(\"Example multi-hot encoded predictions from ClassifierChain:\")\n",
        "display(y_pred_chain[:5])\n",
        "\n",
        "# You can then evaluate y_pred_chain using multi-label metrics\n",
        "# For example, using the compute_metrics function:\n",
        "# metrics = compute_metrics(y_pred_chain, y_test)\n",
        "# print(metrics)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}